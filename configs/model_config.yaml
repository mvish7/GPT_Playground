
num_blocks: 6
num_layers: 8
attention: multi_query  # vanilla, multi_query, group_query
emb_dim: 64
num_heads: 8
block_size: 32
num_groups: 4  # used only for group_query_attention, num_heads % num_groups == 0
vocab_size: 0  # to be set after train dataset creation, for now
is_moe: False  # True or False
num_experts: 4
num_topk: 2
multi_toke_pred:
  do_mtp: True
  total_num_token: 3
  num_tx_head: 2  # num_head for transformer layer inside the MTP module